[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/Subject-Matter-Enrichment/src/main/scala/enrichment.scala","languageId":"scala","version":1,"text":"import org.apache.log4j.Logger\nimport org.apache.flink.api.common.eventtime.WatermarkStrategy\nimport org.apache.flink.api.common.serialization.SimpleStringSchema\nimport org.apache.flink.connector.kafka.source.KafkaSource\nimport org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema\nimport org.apache.flink.connector.kafka.sink.KafkaSink\nimport org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer\nimport org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer\nimport org.apache.flink.streaming.api.functions.sink.SinkFunction\nimport org.apache.flink.types.Row\nimport org.apache.kafka.clients.consumer.{ConsumerRecords, KafkaConsumer}\nimport scala.collection.JavaConversions._\nimport com.datastax.spark.connector._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.cassandra._\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql._\nimport java.util\nimport java.util.Properties\n\n\nobject enrichment extends App {\n  @transient lazy val logger: Logger = Logger.getLogger(getClass.getName)\n\n  val spark = SparkSession.builder()\n      .master(\"local[3]\")\n      .appName(\"enrichment\")\n      .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n      .config(\"spark.sql.shuffle.partitions\", 2)\n      .config(\"spark.cassandra.connection.host\", \"localhost\")\n      .config(\"spark.cassandra.connection.port\", \"9042\")\n      .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n      .config(\"spark.sql.catalog.lh\", \"com.datastax.spark.connector.datasource.CassandraCatalog\")\n      .getOrCreate()\n\n  val teachersDb = spark.read\n      .format(\"org.apache.spark.sql.cassandra\")\n      .option(\"keyspace\", \"flink_db\")\n      .option(\"table\", \"teachers\")\n      .load()\n\n  val env = StreamExecutionEnvironment.getExecutionEnvironment\n\n  val kafkaSource = KafkaSource.builder()\n  .setBootstrapServers(\"localhost:9092\")\n  .setTopics(\"flinkex\")\n  .setGroupId(\"flink-consumer-group\")\n  .setStartingOffsets(OffsetsInitializer.latest())\n  .setValueOnlyDeserializer(new SimpleStringSchema())\n  .build()\n\n  val serializer = KafkaRecordSerializationSchema.builder()\n  .setValueSerializationSchema(new SimpleStringSchema())\n  .setTopic(\"flinkout\")\n  .build()\n\n  val kafkaSink = KafkaSink.builder()\n  .setBootstrapServers(\"localhost:9092\")\n  .setRecordSerializer(serializer)\n  .build()\n\n  val stream2 = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), \"Kafka Source\")\n  val stream1 = stream2.map(x => {\n    val arr = x.split(\",\")\n    val student_id = arr(0)\n    val doubt = arr(1)\n    val domain = arr(2)\n    val foundTeachers = teachersDb.filter(array_contains(col(\"domains\"), domain))\n    import spark.implicits._\n    val teacherIdList: List[String] = foundTeachers.select(\"teacher_id\").as[String].collect().toList\n    val combined = teacherIdList.map(x => {\n      (x, student_id, doubt, domain)\n    })\n    combined\n  })\n  val formattedStream: DataStream[String] = stream1.flatMap(_.map {\n    case (a, b, c, d) => s\"$a,$b,$c,$d\"\n  }).map(_.mkString(\"\"))\n  formattedStream.sinkTo(kafkaSink)\n  env.execute(\"enrichment\")\n\n}\n\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Projects\Subject-Matter-Enrichment\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 2 s, completed 10-Aug-2023, 9:13:57 AM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mForcing garbage collection...[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, â™¨1, shutdown, null})[0m
